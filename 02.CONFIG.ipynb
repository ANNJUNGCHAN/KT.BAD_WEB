{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select to Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패키지 및 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imputation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# main algorithms\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# making event\n",
    "from itertools import product\n",
    "\n",
    "# scoring\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# visualization processing\n",
    "from tqdm import notebook\n",
    "import time\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/bad_web/data/train_dataset.csv')\n",
    "data.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "data['Result_v1'] = data['Result_v1'].apply(lambda x: -1 if x == 'malicious' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 우리에게 필요한 환경에는 어느 것이 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경우의 수 분석하기\n",
    "one_column_select = [True,False] # 하나의 값만 가지는 칼럼을 분석에 적용할 것인지 판단\n",
    "url_len_select = [True,False] # url 길이를 분석에 적용할 것인지 판단\n",
    "url_entropy_select = [True,False] # url entropy를 분석에 적용할 것인지 판단\n",
    "url_port_select = [True,False] # url port를 분석에 적용할 것인지 판단\n",
    "iframe_select = [True,False] # iframe을 분석에 적용할 것인지 판단\n",
    "head_select = [True,False] # head를 분석에 적용할 것인지 판단\n",
    "body_select = [True,False] # body를 분석에 적용할 것인지 판단\n",
    "one_distribute_select = [True,False] # error에만 분포를 가지는 칼럼의 분포를 분석에 적용할 것인지 판단\n",
    "drop_duplicate_select = [True,False] # 중복된 데이터를 제거할 것인지 판단\n",
    "missing_impute_select = [True,False] # missing value를 impute할 것인지 판단\n",
    "scaler_select = [\"NONE\"] # scaler를 적용할 것인지 판단\n",
    "\n",
    "# 모든 경우의 수를 product 연산을 통해 생성하기\n",
    "iter = list(product(one_column_select,url_len_select,url_entropy_select,url_port_select,\n",
    "iframe_select,head_select,body_select,one_distribute_select,\n",
    "drop_duplicate_select,missing_impute_select,scaler_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 셋 모델링 함수 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 모델링 함수 구축\n",
    "def select_dataset(data,config,test_size) :\n",
    "    \"\"\"\n",
    "    config : dict\n",
    "    test_size : float : test_size를 결정\n",
    "    \"\"\"\n",
    "    # 데이터의 인덱스 초기화\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # one_columns에 해당하는 칼럼 정의\n",
    "    one_columns = ['url_chinese_present',\"html_num_tags('applet')\"]\n",
    "    # one_distribute에 해당하는 칼럼 정의\n",
    "    one_distribute = ['url_query_len','url_num_query_para']\n",
    "    # 모든 칼럼에서 경우의 수에 따라서 필요 없는 칼럼을 제거해 나갈 것이다.\n",
    "    all_columns = list(data.columns)\n",
    "    # one_columns에 해당하는 칼럼을 제거할 것인가?\n",
    "    if config['one_column_on'] == False :\n",
    "        for i in one_columns :\n",
    "            all_columns.remove(i)\n",
    "    # url_len을 분석에 적용할 것인가?    \n",
    "    if config['url_len_off'] == False :\n",
    "        all_columns.remove('url_len')\n",
    "    # url_entropy를 분석에 적용할 것인가?\n",
    "    if config['url_entropy_off'] == False :\n",
    "        all_columns.remove('url_entropy')\n",
    "    # url_port를 분석에 적용할 것인가?\n",
    "    if config['url_port_off'] == False :\n",
    "        all_columns.remove('url_port')\n",
    "    # iframe을 분석에 적용할 것인가?\n",
    "    if config['iframe_off'] == False :\n",
    "        all_columns.remove(\"html_num_tags('iframe')\")\n",
    "    # head를 분석에 적용할 것인가?\n",
    "    if config['head_off'] == False :\n",
    "        all_columns.remove(\"html_num_tags('head')\")\n",
    "    # body를 분석에 적용할 것인가?\n",
    "    if config['body_off'] == False :\n",
    "        all_columns.remove(\"html_num_tags('body')\")\n",
    "    # one_distribute에 해당하는 칼럼을 분석에 적용할 것인가?\n",
    "    if config['one_distribute_on'] == False :\n",
    "        for i in one_distribute :\n",
    "            all_columns.remove(i)\n",
    "\n",
    "    # 경우의 수에 따라 정제된 칼럼을 정의함\n",
    "    data = data[all_columns]\n",
    "\n",
    "    # 중복된 데이터를 제거할 것인가?\n",
    "    if config['drop_duplicate'] == True :\n",
    "        data.drop_duplicates(inplace = True)\n",
    "        data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # missing value를 impute할 것인가?\n",
    "    if config['missing_imput'] == True :\n",
    "        X = data.drop(columns = [\"Result_v1\"])\n",
    "        y = data[\"Result_v1\"]\n",
    "        imp = IterativeImputer(estimator = RandomForestRegressor(verbose=0, random_state = 42), # impute를 할 시 InterativeImputer를 사용(MissForest)\n",
    "                               max_iter=10,\n",
    "                               verbose = 0,\n",
    "                               imputation_order= 'ascending',\n",
    "                               random_state=0)\n",
    "        X_imp=pd.DataFrame(imp.fit_transform(X))\n",
    "        X_imp.columns = X.columns\n",
    "        data = pd.concat([X_imp,y], axis = 1)    \n",
    "    else :\n",
    "        data = data.dropna() # impute 하지 않을 시, 모든 missing value를 제거\n",
    "        data = data.reset_index(drop = True)\n",
    "\n",
    "    # train test split\n",
    "    train, test = train_test_split(data, test_size = test_size, random_state = 42,shuffle=True)\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    test.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # 어느 scaler를 적용할 것인가?\n",
    "    if config['scaler'] == \"MinMax\" :\n",
    "        scaler = MinMaxScaler()\n",
    "        train_x = train.drop(columns = [\"Result_v1\"])\n",
    "        column = train_x.columns\n",
    "        train_y = train[\"Result_v1\"]\n",
    "        test_x = test.drop(columns = [\"Result_v1\"])\n",
    "        test_y = test[\"Result_v1\"]\n",
    "        scaler = scaler.fit(train_x)\n",
    "        train_x = scaler.transform(train_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "        train_x = pd.DataFrame(train_x)\n",
    "        test_x = pd.DataFrame(test_x)\n",
    "        train_x.columns = column\n",
    "        test_x.columns = column\n",
    "        train = pd.concat([train_x,train_y],axis = 1)\n",
    "        test = pd.concat([test_x,test_y],axis = 1)\n",
    "    if config['scaler'] == \"Standard\" :\n",
    "        scaler = StandardScaler()\n",
    "        train_x = train.drop(columns = [\"Result_v1\"])\n",
    "        column = train_x.columns\n",
    "        train_y = train[\"Result_v1\"]\n",
    "        test_x = test.drop(columns = [\"Result_v1\"])\n",
    "        test_y = test[\"Result_v1\"]\n",
    "        scaler = scaler.fit(train_x)\n",
    "        train_x = scaler.transform(train_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "        train_x = pd.DataFrame(train_x)\n",
    "        test_x = pd.DataFrame(test_x)\n",
    "        train_x.columns = column\n",
    "        test_x.columns = column\n",
    "        train = pd.concat([train_x,train_y],axis = 1)\n",
    "        test = pd.concat([test_x,test_y],axis = 1)\n",
    "    if config['scaler'] == \"MaxAbs\" :\n",
    "        scaler = MaxAbsScaler()\n",
    "        train_x = train.drop(columns = [\"Result_v1\"])\n",
    "        column = train_x.columns\n",
    "        train_y = train[\"Result_v1\"]\n",
    "        test_x = test.drop(columns = [\"Result_v1\"])\n",
    "        test_y = test[\"Result_v1\"]\n",
    "        scaler = scaler.fit(train_x)\n",
    "        train_x = scaler.transform(train_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "        train_x = pd.DataFrame(train_x)\n",
    "        test_x = pd.DataFrame(test_x)\n",
    "        train_x.columns = column\n",
    "        test_x.columns = column\n",
    "        train = pd.concat([train_x,train_y],axis = 1)\n",
    "        test = pd.concat([test_x,test_y],axis = 1)\n",
    "    if config['scaler'] == \"Robust\" :\n",
    "        scaler = RobustScaler()\n",
    "        train_x = train.drop(columns = [\"Result_v1\"])\n",
    "        column = train_x.columns\n",
    "        train_y = train[\"Result_v1\"]\n",
    "        test_x = test.drop(columns = [\"Result_v1\"])\n",
    "        test_y = test[\"Result_v1\"]\n",
    "        scaler = scaler.fit(train_x)\n",
    "        train_x = scaler.transform(train_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "        train_x = pd.DataFrame(train_x)\n",
    "        test_x = pd.DataFrame(test_x)\n",
    "        train_x.columns = column\n",
    "        test_x.columns = column\n",
    "    if config['scaler'] == \"NONE\" :\n",
    "        train_x = train.drop(columns = [\"Result_v1\"])\n",
    "        train_y = train[\"Result_v1\"]\n",
    "        test_x = test.drop(columns = [\"Result_v1\"])\n",
    "        test_y = test[\"Result_v1\"]\n",
    "    return train_x, train_y, test_x, test_y # 최종적으로 train_x, train_y, test_x, test_y를 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가함수 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_config(one_column,url_len,url_entropy,url_port,iframe,head,body,one_distribute,drop_duplicate,missing_imput,scaler) :\n",
    "    # config 딕셔너리 만들기\n",
    "    config = dict()\n",
    "    config['one_column_on'] = one_column\n",
    "    config['url_len_off'] = url_len\n",
    "    config['url_entropy_off'] = url_entropy\n",
    "    config['url_port_off'] = url_port\n",
    "    config['iframe_off'] = iframe\n",
    "    config['head_off'] = head\n",
    "    config['body_off'] = body\n",
    "    config['one_distribute_on'] = one_distribute\n",
    "    config['drop_duplicate'] = drop_duplicate\n",
    "    config['missing_imput'] = missing_imput\n",
    "    config['scaler'] = scaler\n",
    "    # 환경에 따른 데이터 셋 불러오기\n",
    "    train_x, train_y, test_x, test_y  = select_dataset(data,config,0.2)\n",
    "    # catboost 모델 학습 및 예측\n",
    "    model = CatBoostClassifier(random_state = 42, verbose = 0)\n",
    "    model.fit(train_x, train_y)\n",
    "    predict_y = model.predict(test_x)\n",
    "    # f1 score로 평가\n",
    "    f1 = metrics.f1_score(test_y, predict_y)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in notebook.tqdm(iter) :\n",
    "    time.sleep(0.01)\n",
    "    output = iter_config(i[0],i[1],i[2],i[3],i[4],i[5],i[6],i[7],i[8],i[9],i[10])\n",
    "    result.append(output)\n",
    "    print(i,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임에 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 생성 및 값 넣기\n",
    "final = pd.DataFrame()\n",
    "final[\"iter\"] = iter\n",
    "final[\"f1\"] = result\n",
    "# 스코어가 높은 순서대로 sorting하기 & csv 파일 형태로 저장\n",
    "final.sort_values(by = \"f1\", ascending = False).to_csv(\"result_all.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8c8540e0960871b600e3f40e1e37dd4369b4892e9ee484b4784a47d7408b04b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
